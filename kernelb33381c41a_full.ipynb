{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime, timedelta\nimport json\nimport pickle as pkl\n\nfrom sklearn import tree\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\nspecs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prev_assessment_accuracy(df):\n    try:\n        acc = df[df['type'] == 'Assessment'].tail(1)['accuracy'].item()\n    except:\n        acc = np.nan\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_events_during_school_hours(df):\n    start_time = datetime(2019, 9, 5, 7, 45).time()\n    end_time = datetime(2019, 9, 6, 15, 30).time()\n    df.index = df['timestamp']\n    df = df.between_time(start_time, end_time)\n    return len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def total_number_of_misses(df):\n    total_number_of_misses = (df['num_correct'] == 0).sum()\n    return total_number_of_misses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentage_of_misses(df):\n    percentage_misses = total_number_of_misses(df) / df['num_correct'].notnull()\n    return percentage_misses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_exit_type_other_than_gamecompleted(df):\n    \"\"\"1 if never had an exit_type other than 'game_completed', and 0 otherwise\"\"\"\n    a = df['event_data'].apply(json.loads).apply(lambda d: d['exit_type'] if 'exit_type' in d else np.nan)\n    a = a[a.notnull()]\n    if len(a) == 0:\n        return 0\n    if (a == 'game_completed').all():\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_events_correct_json(df):\n    \"\"\"Count number of events that say 'correct':true\"\"\"\n    a = df['event_data'].apply(json.loads).apply(lambda d: int(d['correct']) if 'correct' in d else np.nan)\n    a = a[a.notnull()]\n    if len(a) == 0:\n        return 0\n    else:\n        return a.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_events_incorrect_json(df):\n    \"\"\"Count number of events that say 'correct':false\"\"\"\n    a = df['event_data'].apply(json.loads).apply(lambda d: int(not d['correct']) if 'correct' in d else np.nan)\n    a = a[a.notnull()]\n    if len(a) == 0:\n        return 0\n    else:\n        return a.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_game_play_total_time(df):\n    session_times = df.groupby('game_session')['timestamp']\n    return (session_times.max() - session_times.min()).sum().total_seconds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_wild_click_sessions(df, clicks, seconds):\n    \"\"\"A wild click session is defined as a spurt of at least n clicks in m seconds.\n    Warning: this will measure long click sessions as multiple shorter ones.\n    \n    ARGS: \n    df -- (DataFrame) the user's complete history\n    clicks -- (int) number of clicks in the clickspurt.\n    seconds -- (float) number of seconds that define the span of a clickspurt.\n    \n    RETURNS: the amount of individual wild click sessions in player history\n    \"\"\"\n    return (df['timestamp'].diff(clicks) < timedelta(seconds=seconds)).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def longest_wild_click_session(df, clicks, seconds):\n    \"\"\"A wild click session (WCS) is defined as a spurt of at least n clicks in m seconds.\n    If you consider consecutive WCSs as a single, longer WCS, you can measure the duration of them.\n    This function returns the duration of the longest WCS in a user's history,\n    and the number of WCSs the user had if you treatconsecutive WCSs as a single, longer WCS.\n    \n    ARGS: \n    df -- (DataFrame) the user's complete history\n    clicks -- (int) number of clicks in the clickspurt.\n    seconds -- (float) number of seconds that define the span of a clickspurt.\n    \n    RETURNS: \n    longest_run -- (int) longest run of consecutive WCSs\n    number_of_runs -- (int) number of WCSs if you treat consecutive ones as a single WCS\n    \"\"\"\n    df = df.reset_index(drop=True)  # just in case there are two rows with the same index label\n    wcs_rows = (df['timestamp'].diff(clicks) < timedelta(seconds=seconds))\n    wcs_rows = wcs_rows[wcs_rows].index  # get all the locations where a single WCS occurred\n    wcs_locations = pd.Series(df.loc[wcs_rows,:].index)\n    location_diffs = wcs_locations.diff(1).iloc[1:]  # the first value is null\n\n    longest_run, number_of_runs, current_run = 0, 0, 0\n    for val in location_diffs:\n        if val == 1:\n            current_run += 1\n        else:\n            longest_run = max(longest_run, current_run)\n            if current_run > 0:\n                number_of_runs += 1\n            current_run = 0\n\n    return longest_run, number_of_runs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def length_of_current_play_session(df, break_minutes=30):\n    \"\"\"A play session is active until there are no events for at least m minutes.\"\"\"\n    df = df.reset_index(drop=True)\n    break_start_locations = (df['timestamp'].diff(1) > timedelta(minutes=break_minutes)).reset_index(drop=True)\n    break_start_locations.iloc[0] = True\n    break_start_locations = break_start_locations[break_start_locations].index\n    break_end_locations = pd.Series(break_start_locations - 1).iloc[1:]\n    break_end_locations = break_end_locations.append(pd.Series([len(df) - 1])).reset_index(drop=True)\n    session_durations = df.loc[break_end_locations, 'timestamp'].reset_index(drop=True) - df.loc[break_start_locations, 'timestamp'].reset_index(drop=True)\n    return session_durations.iloc[-1].total_seconds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_play_sessions(df, break_minutes=30):\n    \"\"\"A play session is active until there are no events for at least m minutes.\"\"\"\n    df = df.reset_index(drop=True)\n    break_start_locations = (df['timestamp'].diff(1) > timedelta(minutes=break_minutes)).reset_index(drop=True)\n    break_start_locations.iloc[0] = True\n    break_start_locations = break_start_locations[break_start_locations].index\n    break_end_locations = pd.Series(break_start_locations - 1).iloc[1:]\n    break_end_locations = break_end_locations.append(pd.Series([len(df) - 1])).reset_index(drop=True)\n    session_durations = df.loc[break_end_locations, 'timestamp'].reset_index(drop=True) - df.loc[break_start_locations, 'timestamp'].reset_index(drop=True)\n    return len(session_durations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def current_play_session_compared_to_mean(df, break_minutes=30):\n    df = df.reset_index(drop=True)\n    break_start_locations = (df['timestamp'].diff(1) > timedelta(minutes=break_minutes)).reset_index(drop=True)\n    break_start_locations.iloc[0] = True\n    break_start_locations = break_start_locations[break_start_locations].index\n    break_end_locations = pd.Series(break_start_locations - 1).iloc[1:]\n    break_end_locations = break_end_locations.append(pd.Series([len(df) - 1])).reset_index(drop=True)\n    session_durations = df.loc[break_end_locations, 'timestamp'].reset_index(drop=True) - df.loc[break_start_locations, 'timestamp'].reset_index(drop=True)\n    mean_session_duration = session_durations.mean()\n    return (mean_session_duration.total_seconds() - session_durations.iloc[-1].total_seconds())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def current_play_session_compared_to_median(df, break_minutes=30):\n    df = df.reset_index(drop=True)\n    break_start_locations = (df['timestamp'].diff(1) > timedelta(minutes=break_minutes)).reset_index(drop=True)\n    break_start_locations.iloc[0] = True\n    break_start_locations = break_start_locations[break_start_locations].index\n    break_end_locations = pd.Series(break_start_locations - 1).iloc[1:]\n    break_end_locations = break_end_locations.append(pd.Series([len(df) - 1])).reset_index(drop=True)\n    session_durations = df.loc[break_end_locations, 'timestamp'].reset_index(drop=True) - df.loc[break_start_locations, 'timestamp'].reset_index(drop=True)\n    mean_session_duration = session_durations.median()\n    return (mean_session_duration.total_seconds() - session_durations.iloc[-1].total_seconds())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_global_session_lengths(df):\n    global_session_lengths = df.groupby(['installation_id', 'game_session'])[['title', 'game_time', 'event_count']].max()\n    global_session_lengths[global_session_lengths['game_time'] > 0]\n    global_session_lengths = global_session_lengths.groupby('title')[['game_time', 'event_count']]\n    global_mean_session_lengths = global_session_lengths.mean()\n    global_median_session_lengths = global_session_lengths.median()\n    return global_mean_session_lengths, global_median_session_lengths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_player_vs_global_features(df, global_mean_session_lengths, global_median_session_lengths):\n    session_lengths = df.groupby('game_session')[['title', 'game_time', 'event_count']].max()\n    session_lengths = session_lengths[session_lengths['game_time'] > 0]  # ignore sessions w/ no duration\n    player_session_lengths = session_lengths.groupby('title')[['game_time', 'event_count']]\n    player_mean_session_lengths = player_session_lengths.mean()\n    player_median_session_lengths = player_session_lengths.median()\n    \n    player_vs_global_mean = pd.merge(player_mean_session_lengths, global_mean_session_lengths, \n                                on='title', how='inner', suffixes=['_player', '_global'])\n    p_v_g_game_time_mean = player_vs_global_mean['game_time_player'] - player_vs_global_mean['game_time_global']\n    p_v_g_event_count_mean = player_vs_global_mean['event_count_player'] - player_vs_global_mean['event_count_global']\n    p_v_g_gt_sum_mean = p_v_g_game_time_mean.sum()\n    p_v_g_ec_sum_mean = p_v_g_event_count_mean.sum()\n    \n    player_vs_global_median = pd.merge(player_median_session_lengths, global_median_session_lengths, \n                                on='title', how='inner', suffixes=['_player', '_global'])\n    p_v_g_game_time_median = player_vs_global_median['game_time_player'] - player_vs_global_median['game_time_global']\n    p_v_g_event_count_median = player_vs_global_median['event_count_player'] - player_vs_global_median['event_count_global']\n    p_v_g_gt_sum_median = p_v_g_game_time_median.sum()\n    p_v_g_ec_sum_median = p_v_g_event_count_median.sum()\n    return (p_v_g_gt_sum_mean, p_v_g_ec_sum_mean, \n            p_v_g_gt_sum_median, p_v_g_ec_sum_median)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def events_from_specs():\n    df = specs\n    correct_events = df[(df['info'].str.contains('\\(Correct\\)', case=False, na=False)) | (df['info'].str.contains(' correct', case=False, na=False))]\n    correct_events = set(correct_events['event_id'])\n    incorrect_events = df[(df['info'].str.contains('\\(Inorrect\\)', case=False, na=False)) | (df['info'].str.contains('incorrect', case=False, na=False))]\n    incorrect_events = set(incorrect_events['event_id'])\n    return correct_events, incorrect_events","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_events_correct_eventid(df, correct_events):\n    return df['event_id'].isin(correct_events).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_events_incorrect_eventid(df, incorrect_events):\n    return df['event_id'].isin(incorrect_events).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def times_took_assess(df):\n    df_1 = df.groupby([\"installation_id\", \"title\"]).transform('count')\n    df_2= df_1[[\"event_id\"]]\n    df_2 = df_2.rename(columns={\"event_id\": \"times_played\"})\n    df = df.merge(df_2, left_index=True, right_index=True)\n    return df[\"times_played\"].iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_all_features(df):\n    feature = {'installation_id': installation_id,\n               'worlds_played': max(df['world'].nunique(), 0),\n               'time_as_player': max((df.iloc[-1]['timestamp'] - df.iloc[0]['timestamp']).total_seconds(), 0),\n               'num_assessments': max((df['type'] == 'Assessment').sum(), 0),\n               'avg_assessment_time': max(df[df['type'] == 'Assessment']['game_time'].mean(), 0),\n               'tot_time_playing_game': max(df['game_time'].sum(), 0),\n               'prev_assessment_accuracy': get_prev_assessment_accuracy(df),\n               'num_events_during_school_hours': num_events_during_school_hours(df),\n               'exit_type_other_than_gamecompleted': calc_exit_type_other_than_gamecompleted(df),\n               'game_play_total_time': calc_game_play_total_time(df),\n               'num_wild_click_sessions': num_wild_click_sessions(df, 5, 0.9),\n               'num_wild_click_sessions_grouped': num_wild_click_sessions_grouped,\n               'longest_wild_click_run': longest_wild_click_run,\n               'length_of_current_play_session': length_of_current_play_session(df, 30),\n               'num_play_sessions': num_play_sessions(df, 30),\n               'current_play_session_compared_to_mean': current_play_session_compared_to_mean(df, 30),\n               'current_play_session_compared_to_median': current_play_session_compared_to_median(df, 30),\n               'p_v_g_gt_sum_mean': p_v_g_gt_sum_mean,\n               'p_v_g_ec_sum_mean': p_v_g_ec_sum_mean,\n               'p_v_g_gt_sum_median': p_v_g_gt_sum_median,\n               'p_v_g_ec_sum_median': p_v_g_ec_sum_median,\n               'num_events_correct_json': num_events_correct_json(df),\n               'num_events_incorrect_json': num_events_correct_json(df),\n               'num_events_correct_eventid': num_events_correct_eventid(df, correct_events),\n               'num_events_incorrect_eventid': num_events_incorrect_eventid(df, incorrect_events),\n               'part_of_day': max(df[\"segment_of_day\"].iloc[-1], 0),\n               'assessment_taken': max(df[\"assessment\"].iloc[-1], 0),                \n               'time_playing_for': max(df[\"game_time\"].iloc[-1], 0),\n               'times_took_asses': times_took_assess(df),\n               '2010': max(df[2010].iloc[-1], 0),\n               '2020': max(df[2020].iloc[-1], 0),\n               '2025': max(df[2025].iloc[-1], 0),\n               '2030': max(df[2030].iloc[-1], 0),\n               '2035': max(df[2035].iloc[-1], 0),\n               '3010': max(df[3010].iloc[-1], 0),\n               '3020': max(df[3020].iloc[-1], 0),\n               '3021': max(df[3021].iloc[-1], 0),\n               '3110': max(df[3110].iloc[-1], 0),\n               '3120': max(df[3120].iloc[-1], 0),\n               '3121': max(df[3121].iloc[-1], 0),\n               '4020': max(df[4020].iloc[-1], 0),\n               '4025': max(df[4025].iloc[-1], 0),\n               '4030': max(df[4030].iloc[-1], 0),\n               '4035': max(df[4035].iloc[-1], 0),\n               '4040': max(df[4040].iloc[-1], 0),\n               '4070': max(df[4070].iloc[-1], 0),\n               '4080': max(df[4080].iloc[-1], 0),\n               '4090': max(df[4090].iloc[-1], 0),\n               '4100': max(df[4100].iloc[-1], 0),\n               '4110': max(df[4110].iloc[-1], 0)\n#                'total_number_of_misses': total_number_of_misses(df),\n#                'percentage_of_misses': percentage_of_misses(df),\n#                'avg_assessment_accuracy': df[df['type'] == 'Assessment']['accuracy'].mean(),\n#                'total_correct': max(df['num_correct'].sum(), 0),\n#                'total_incorrect': max(df['num_incorrect'].sum(), 0),\n#                'playtime_vs_avg': time_compared_to_normal(df),\n              }\n    return feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN - pre-processing\nassessed_ids = train[train['type'] == 'Assessment']['installation_id'].unique()\ntrain = train[train['installation_id'].isin(assessed_ids)]\ntrain.shape\n\nlabeled_ids = train_labels['installation_id'].unique()\ntrain = train[train['installation_id'].isin(labeled_ids)]\n\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntrain.sort_values(['installation_id', 'timestamp'], inplace=True)\n\ntrain = pd.merge(train, train_labels, on=['installation_id', 'game_session'], how='left')\n\ntrain_cuts = train[(train['event_code'] == 2000) & \n                   (train['type'] == 'Assessment') & \n                   (train['accuracy_group'].notnull())]\n\n# TRAIN - global calculations for features\nglobal_mean_session_lengths, global_median_session_lengths = calc_global_session_lengths(train)\ncorrect_events, incorrect_events = events_from_specs()\n\na = train[[\"installation_id\", \"event_code\"]]\na1 = a.pivot_table(index='installation_id', columns='event_code', aggfunc=len, fill_value=0)\na1 = a1[[2010, 2020,2025, 2030, 2035,3010, 3020, 3021,3110, 3120, 3121,4020, 4025, 4030,4035, 4040, 4070,4080, 4090,4100,4110]]\na1[\"installation_id\"] = a1.index\na1.reset_index(drop=True, inplace=True)\ntrain = pd.merge(train, a1, on=['installation_id'], how='left')\n\ntrain[\"time_of_day\"] = train[\"timestamp\"].astype(str).str[11:13]\ntrain[\"time_of_day\"] = train[\"time_of_day\"].astype(int)\ntrain[\"segment_of_day\"] = np.where(train[\"time_of_day\"]<7, 1.524239, \n                          np.where(train[\"time_of_day\"]<12, 1.746823, \n                           np.where(train[\"time_of_day\"]<18, 1.556186, 1.502395)))\n\ntrain[\"assessment\"] = np.where(train[\"title_x\"]==\"Bird Measurer (Assessment)\", 1.14, \n                      np.where(train[\"title_x\"]==\"Cart Balancer (Assessment)\", 1.86, \n                       np.where(train[\"title_x\"]==\"Cauldron Filler (Assessment)\", 2.08, \n                        np.where(train[\"title_x\"]==\"Chest Sorter (Assessment)\", 0.67, \n                         np.where(train[\"title_x\"]==\"Bird Measurer (Assessment)\", 1.97, float(\"nan\"))))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN - calculate features\ncount = 0\nX, y = [], []\nfor i, row in train_cuts.iterrows():\n    count += 1\n    installation_id, game_session = row['installation_id'], row['game_session']\n    df = train[train['installation_id'] == installation_id]\n    # get the timestamp of the cut row\n    cut_time = df.loc[i,'timestamp']\n    # cut the df\n    df = df[df['timestamp'] <= cut_time]\n    if df.empty:\n        continue\n    df['accuracy_group'] = row['accuracy_group']\n    \n    # added player calculations, used to create features below\n    longest_wild_click_run, num_wild_click_sessions_grouped = longest_wild_click_session(df, 5, 0.9)\n    p_v_g_stats = calc_player_vs_global_features(df, global_mean_session_lengths, global_median_session_lengths)\n    p_v_g_gt_sum_mean, p_v_g_ec_sum_mean, p_v_g_gt_sum_median, p_v_g_ec_sum_median = p_v_g_stats\n    \n    feature = calc_all_features(df)\n    \n    X.append(feature)\n    y.append(row['accuracy_group'])\n    \n    if count % 1000 == 0:\n        print('progress = {}%'.format(count/17690*100))\n    \n#     if count > 2:  # note we're just making features for the first n cuts\n#         break\n\nX = pd.DataFrame(X)\nX.to_csv('all_features.csv', index=False)\nX.shape, len(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN - fill missing values\nfill_vals = {'assessment_taken': 0,\n             'prev_assessment_accuracy': 0}\nX.fillna(fill_vals, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN - augment data to even out the classes\nX_y = pd.concat([X, pd.Series(y, name='labels')], axis=1)\n\nX_y['labels'] = X_y['labels'].astype('int64')\nzeros = X_y[X_y['labels'] == 0]\nones = X_y[X_y['labels'] == 1]\ntwos = X_y[X_y['labels'] == 2]\n\nX_y = pd.concat([X_y, zeros, ones, ones, ones, twos, twos, twos])\nX = X_y[[col for col in X_y.columns if col not in ['labels']]]\ny = X_y['labels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['timestamp'] = pd.to_datetime(test['timestamp'])\ntest.sort_values(['installation_id', 'timestamp'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST - added global calculations, used to create features\n# global_mean_session_lengths, global_median_session_lengths = calc_global_session_lengths(pd.concat([train, test]))\n# correct_events, incorrect_events = events_from_specs()\n\na = test[[\"installation_id\", \"event_code\"]]\na1 = a.pivot_table(index='installation_id', columns='event_code', aggfunc=len, fill_value=0)\na1 = a1[[2010, 2020,2025, 2030, 2035,3010, 3020, 3021,3110, 3120, 3121,4020, 4025, 4030,4035, 4040, 4070,4080, 4090,4100,4110]]\na1[\"installation_id\"] = a1.index\na1.reset_index(drop=True, inplace=True)\ntest = pd.merge(test, a1, on=['installation_id'], how='left')\n\ntest[\"time_of_day\"] = test[\"timestamp\"].astype(str).str[11:13]\ntest[\"time_of_day\"] = test[\"time_of_day\"].astype(int)\ntest[\"segment_of_day\"] = np.where(test[\"time_of_day\"]<7, 1.524239, \n                          np.where(test[\"time_of_day\"]<12, 1.746823, \n                           np.where(test[\"time_of_day\"]<18, 1.556186, 1.502395)))\n\ntest[\"assessment\"] = np.where(test[\"title\"]==\"Bird Measurer (Assessment)\", 1.14, \n                      np.where(test[\"title\"]==\"Cart Balancer (Assessment)\", 1.86, \n                       np.where(test[\"title\"]==\"Cauldron Filler (Assessment)\", 2.08, \n                        np.where(test[\"title\"]==\"Chest Sorter (Assessment)\", 0.67, \n                         np.where(test[\"title\"]==\"Bird Measurer (Assessment)\", 1.97, float(\"nan\"))))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST - create features\ncount = 0\nX, y = [], []\nfor installation_id in test['installation_id'].unique():\n    count += 1\n    df = test[test['installation_id'] == installation_id]\n    \n    # added player calculations, used to create features below\n    longest_wild_click_run, num_wild_click_sessions_grouped = longest_wild_click_session(df, 5, 0.9)\n    p_v_g_stats = calc_player_vs_global_features(df, global_mean_session_lengths, global_median_session_lengths)\n    p_v_g_gt_sum_mean, p_v_g_ec_sum_mean, p_v_g_gt_sum_median, p_v_g_ec_sum_median = p_v_g_stats\n    \n    feature = calc_all_features(df)\n    \n    X.append(feature)\n    \n    if count % 100 == 0:\n        print('progress = {}%'.format(count/1000*100))\n    \n#     if count > 2:  # note we're just making features for the first n cuts\n#         break\n\nX = pd.DataFrame(X)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_vals = {'assessment_taken': 0,\n             'prev_assessment_accuracy': 0}\nX.fillna(fill_vals, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [col for col in X.columns if col not in ['installation_id']]\n\nX_1 = np.array(X[cols])\nX['accuracy_group'] = clf.predict(X_1)\n\nsubmission = X[['installation_id', 'accuracy_group']]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}