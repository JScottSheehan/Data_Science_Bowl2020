{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "train_labels = pd.read_csv('data/train_labels.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11341042, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya don't need 'em if they didn't take an Assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8294138, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessed_ids = train[train['type'] == 'Assessment']['installation_id'].unique()\n",
    "train = train[train['installation_id'].isin(assessed_ids)]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya don't need 'em if they ain't got no labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7734558, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_ids = train_labels['installation_id'].unique()\n",
    "train = train[train['installation_id'].isin(labeled_ids)]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn `timestamp` into a datetime (for sorting purposes, just to be safe), and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "train.sort_values(['installation_id', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "- not all assessments w/in a single `installation_id` have labels \n",
    "- in the test set, for each `installation_id` a random assessment is picked and you have to evaluate that one, so you can only use data prior to that assessment to make a prediction\n",
    "- when training, make features out of all data prior to an assessment (if assessment has a label), then attach the label of that assessment to make `X` and `y`.\n",
    "    - think: \"what would I have guessed if the data was cut at this assessment\"\n",
    "- therefore, you can make multiple predictions per `installation_id`, one prediction per combination of `installation_id` and `game_session` (only if game_session has a label)\n",
    "\n",
    "- every assessment begins with `event_code == 2000` and `type == Assessment`, so this is where we need to cut the data off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "challenge: we need to evaluate the data for each `installation_id+game_session` pair for each cut\n",
    "- this is going to take a long ass time\n",
    "    - how can we do it quicker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge the `train` df w/ the `train_labels` df b/c we want to make sure we cut only where there is a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = train.copy()  # just in case we need the original data\n",
    "\n",
    "train = pd.merge(train, train_labels, on=['installation_id', 'game_session'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the `train` data using a boolean mask, this is what we'll loop through (slowly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cuts = train[(train['event_code'] == 2000) & \n",
    "                   (train['type'] == 'Assessment') & \n",
    "                   (train['accuracy_group'].notnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many loop iterations? count the combos of `installation_id` and `game_session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17690, 17690)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos = train_cuts['installation_id'] + train_cuts['game_session']\n",
    "combos.nunique(), combos.count()  # make sure there are no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress = 5.652911249293386%\n",
      "progress = 11.305822498586773%\n",
      "progress = 16.95873374788016%\n",
      "progress = 22.611644997173546%\n",
      "progress = 28.26455624646693%\n",
      "progress = 33.91746749576032%\n",
      "progress = 39.5703787450537%\n",
      "progress = 45.22328999434709%\n",
      "progress = 50.87620124364047%\n",
      "progress = 56.52911249293386%\n",
      "progress = 62.182023742227244%\n",
      "progress = 67.83493499152064%\n",
      "progress = 73.48784624081401%\n",
      "progress = 79.1407574901074%\n",
      "progress = 84.7936687394008%\n",
      "progress = 90.44657998869418%\n",
      "progress = 96.09949123798756%\n",
      "CPU times: user 1h 45min 2s, sys: 5.07 s, total: 1h 45min 7s\n",
      "Wall time: 1h 45min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "X, y = [], []\n",
    "for i, row in train_cuts.iterrows():\n",
    "    count += 1\n",
    "    installation_id, game_session = row['installation_id'], row['game_session']\n",
    "    df = train[train['installation_id'] == installation_id]\n",
    "    # get the timestamp of the cut row\n",
    "    cut_time = df.loc[i,'timestamp']\n",
    "    # cut the df\n",
    "    df = df[df['timestamp'] <= cut_time]\n",
    "    # cut off last row (assumes df is sorted by time)\n",
    "    df = df.iloc[:-1,:]\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df['accuracy_group'] = row['accuracy_group']\n",
    "    \n",
    "    feature = {'worlds_played': max(df['world'].nunique(), 0),\n",
    "               'total_correct': max(df['num_correct'].sum(), 0),\n",
    "               'total_incorrect': max(df['num_incorrect'].sum(), 0),\n",
    "               'time_as_player': max((df.iloc[-1]['timestamp'] - df.iloc[0]['timestamp']).total_seconds(), 0),\n",
    "               'num_assessments': max((df['type'] == 'Assessment').sum(), 0)}\n",
    "    \n",
    "    X.append(feature)\n",
    "    y.append(row['accuracy_group'])\n",
    "    \n",
    "    if count % 1000 == 0:\n",
    "        print('progress = {}%'.format(count/17690*100))\n",
    "    \n",
    "#     if count > 250:  # note we're just making features for the first n cuts\n",
    "#         break\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X.shape, len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: there is some loss here (users with no data) and we need to figure out how to handle these cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17577, 17577)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5 -- kappa_score: 0.09208646262125941\n",
      "Run 2/5 -- kappa_score: 0.09989737711909585\n",
      "Run 3/5 -- kappa_score: 0.07534410587590434\n",
      "Run 4/5 -- kappa_score: 0.07416316783009469\n",
      "Run 5/5 -- kappa_score: 0.07852025915004468\n",
      "\n",
      "mean score: 0.0840022745192798\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "total_runs = skf.get_n_splits()\n",
    "scores = []\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    count += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    score = cohen_kappa_score(clf.predict(X_test), y_test, weights='quadratic')\n",
    "    scores.append(score)\n",
    "    print('Run {}/{} -- kappa_score: {}'.format(count, total_runs, score))\n",
    "print('\\nmean score: {}'.format(sum(scores)/len(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean time in an assessment (exclude outliers?)\n",
    "- cumulative time playing the game?\n",
    "- installation duration mean (what does this mean? same as above?)\n",
    "- accuracy of game/activity directly prior to the assessment\n",
    "- has it been a long gaming session?\n",
    "- \"exit_type\":\"game_completed\" -- if a player has any other exit types, they are a bad player\n",
    "- event_id=cdd22e43 -- this event could show a player is unskilled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
